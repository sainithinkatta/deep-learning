{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sainithinkatta/deep_learning_class/blob/main/DL_Project_V1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94KXjPppF1TU",
        "outputId": "60efd160-43ab-4ec2-fccd-ae03cd99ee5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "# Path to your zip file (adjust if needed)\n",
        "zip_path = '/content/drive/MyDrive/Colab Notebooks/MOT16.zip'\n",
        "extract_path = '/content/MOT16'\n",
        "\n",
        "# Unzip the dataset\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)"
      ],
      "metadata": {
        "id": "d6m6AlWeIjm0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Check if train/test folders exist\n",
        "print(\"Train sequences:\", os.listdir(\"/content/MOT16/train\"))\n",
        "print(\"Test sequences:\", os.listdir(\"/content/MOT16/test\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFCLz3uPL1hI",
        "outputId": "d9e76bab-72e1-4982-edd1-a214e9f5dd0e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train sequences: ['MOT16-04', 'MOT16-09', 'MOT16-05', 'MOT16-13', 'MOT16-10', 'MOT16-11', 'MOT16-02']\n",
            "Test sequences: ['MOT16-12', 'MOT16-14', 'MOT16-01', 'MOT16-07', 'MOT16-08', 'MOT16-06', 'MOT16-03']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# 2. Imports\n",
        "# ===================================================================\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import AdamW, Adam\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor"
      ],
      "metadata": {
        "id": "XFU7m5gWlHi8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# 4. Parse MOT16 Ground‑Truth\n",
        "# ============================================================================\n",
        "def parse_gt_file(file_path):\n",
        "    data = []\n",
        "    with open(file_path, \"r\") as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split(\",\")\n",
        "            if len(parts) < 6:\n",
        "                continue\n",
        "            f_id, obj_id, x, y, w, h = parts[:6]\n",
        "            data.append({\n",
        "                \"frame_id\":   int(float(f_id)),\n",
        "                \"object_id\":  int(float(obj_id)),\n",
        "                \"bb_left\":    float(x),\n",
        "                \"bb_top\":     float(y),\n",
        "                \"bb_width\":   float(w),\n",
        "                \"bb_height\":  float(h),\n",
        "            })\n",
        "    return data"
      ],
      "metadata": {
        "id": "8T-OtdQHlJU2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# 5. Dataset & DataLoader for Detector (Faster R‑CNN)\n",
        "# ===================================================================\n",
        "class MOTDataset(Dataset):\n",
        "    def __init__(self, img_dir, gt_path, transform=None):\n",
        "        self.img_dir   = img_dir\n",
        "        self.gt_data   = parse_gt_file(gt_path)\n",
        "        self.transform = transform\n",
        "        # collect unique frame IDs\n",
        "        self.frame_ids = sorted({d[\"frame_id\"] for d in self.gt_data})\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.frame_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        fid = self.frame_ids[idx]\n",
        "        img = Image.open(os.path.join(self.img_dir, f\"{fid:06d}.jpg\")).convert(\"RGB\")\n",
        "\n",
        "        boxes, labels = [], []\n",
        "        for obj in self.gt_data:\n",
        "            if obj[\"frame_id\"] != fid:\n",
        "                continue\n",
        "            x1 = obj[\"bb_left\"]\n",
        "            y1 = obj[\"bb_top\"]\n",
        "            x2 = x1 + obj[\"bb_width\"]\n",
        "            y2 = y1 + obj[\"bb_height\"]\n",
        "            boxes.append([x1, y1, x2, y2])\n",
        "            labels.append(1)  # only “person” class\n",
        "\n",
        "        target = {\n",
        "            \"boxes\":  torch.tensor(boxes, dtype=torch.float32),\n",
        "            \"labels\": torch.tensor(labels, dtype=torch.int64),\n",
        "        }\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, target\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "# create transforms & loader\n",
        "det_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "det_dataset   = MOTDataset(\n",
        "    img_dir=  \"/content/MOT16/train/MOT16-02/img1\",\n",
        "    gt_path=  \"/content/MOT16/train/MOT16-02/gt/gt.txt\",\n",
        "    transform=det_transform\n",
        ")\n",
        "det_loader    = DataLoader(\n",
        "    det_dataset,\n",
        "    batch_size=2,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn\n",
        ")\n"
      ],
      "metadata": {
        "id": "aB8m4Octd53o"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# 6. Build & Fine‑Tune the Faster R‑CNN Detector\n",
        "# ===================================================================\n",
        "# helper to swap in our 2‑class head\n",
        "\n",
        "import torch\n",
        "\n",
        "# choose CUDA if available, otherwise fall back to CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def get_detector(num_classes=2):\n",
        "    model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "    in_feats = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_feats, num_classes)\n",
        "    return model.to(device)\n",
        "\n",
        "# instantiate\n",
        "model_det = get_detector(2)\n",
        "optimizer_det = AdamW(\n",
        "    [p for p in model_det.parameters() if p.requires_grad],\n",
        "    lr=5e-4\n",
        ")\n",
        "\n",
        "# training loop\n",
        "num_epochs_det = 5\n",
        "for epoch in range(num_epochs_det):\n",
        "    model_det.train()\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    for imgs, targets in det_loader:\n",
        "        imgs    = [img.to(device) for img in imgs]\n",
        "        targets = [{k: v.to(device) for k,v in t.items()} for t in targets]\n",
        "\n",
        "        loss_dict = model_det(imgs, targets)\n",
        "        loss      = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        optimizer_det.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer_det.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    print(f\"[Detector] Epoch {epoch+1}/{num_epochs_det}  Loss: {epoch_loss/len(det_loader):.4f}\")\n",
        "\n",
        "# save weights\n",
        "torch.save(model_det.state_dict(), \"fasterrcnn_finetuned.pth\")\n",
        "print(\"🔖 Detector weights saved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3-mhSnBd8H4",
        "outputId": "a8037731-ad34-47aa-a53a-fcf9607c1b0f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Detector] Epoch 1/5  Loss: 1.1970\n",
            "[Detector] Epoch 2/5  Loss: 0.6677\n",
            "[Detector] Epoch 3/5  Loss: 0.4930\n",
            "[Detector] Epoch 4/5  Loss: 0.4117\n",
            "[Detector] Epoch 5/5  Loss: 0.3601\n",
            "🔖 Detector weights saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# 7. Dataset for Siamese Network (Re‑ID)\n",
        "# ===================================================================\n",
        "class SiameseDataset(Dataset):\n",
        "    def __init__(self, img_dir, gt_path, transform=None, image_size=(16,16)):\n",
        "        self.img_dir    = img_dir\n",
        "        self.gt_data    = parse_gt_file(gt_path)\n",
        "        self.frame_ids  = sorted({d[\"frame_id\"] for d in self.gt_data})\n",
        "        self.transform  = transform\n",
        "        self.image_size = image_size\n",
        "\n",
        "        # group by frame for fast lookup\n",
        "        self.frames = {}\n",
        "        for d in self.gt_data:\n",
        "            self.frames.setdefault(d[\"frame_id\"], []).append(d)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.frame_ids) * 10  # arbitrary\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # sample two different frames\n",
        "        f1, f2 = np.random.choice(self.frame_ids, 2, replace=False)\n",
        "        objs1, objs2 = self.frames[f1], self.frames[f2]\n",
        "\n",
        "        # decide pos vs neg pair\n",
        "        common = list({o[\"object_id\"] for o in objs1} & {o[\"object_id\"] for o in objs2})\n",
        "        if common and np.random.rand() < 0.5:\n",
        "            oid = np.random.choice(common)\n",
        "            o1 = next(o for o in objs1 if o[\"object_id\"]==oid)\n",
        "            o2 = next(o for o in objs2 if o[\"object_id\"]==oid)\n",
        "            label = 1.0\n",
        "        else:\n",
        "            o1    = objs1[np.random.randint(len(objs1))]\n",
        "            neg   = [o for o in objs2 if o[\"object_id\"]!=o1[\"object_id\"]]\n",
        "            o2    = neg[np.random.randint(len(neg))]\n",
        "            label = 0.0\n",
        "\n",
        "        def crop_obj(o, fid):\n",
        "            img = Image.open(os.path.join(self.img_dir, f\"{fid:06d}.jpg\")).convert(\"RGB\")\n",
        "            x1,y1 = int(o[\"bb_left\"]), int(o[\"bb_top\"])\n",
        "            x2,y2 = x1+int(o[\"bb_width\"]), y1+int(o[\"bb_height\"])\n",
        "            return img.crop((x1,y1,x2,y2)).resize(self.image_size)\n",
        "\n",
        "        img1 = crop_obj(o1, f1)\n",
        "        img2 = crop_obj(o2, f2)\n",
        "        if self.transform:\n",
        "            img1 = self.transform(img1)\n",
        "            img2 = self.transform(img2)\n",
        "\n",
        "        return img1, img2, torch.tensor(label, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "sRMPBN_Ud-SN"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# 8. Define Siamese Network & Contrastive Loss\n",
        "# ===================================================================\n",
        "class ContrastiveLoss(nn.Module):\n",
        "    def __init__(self, margin=1.0):\n",
        "        super().__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, out1, out2, label):\n",
        "        dist = F.pairwise_distance(out1, out2)\n",
        "        return torch.mean((1-label)*dist**2 + label * F.relu(self.margin - dist)**2)\n",
        "\n",
        "class SiameseNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 3), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(64,128,3), nn.ReLU(), nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(128*2*2, 256), nn.ReLU(),\n",
        "            nn.Linear(256, 256)\n",
        "        )\n",
        "\n",
        "    def forward_once(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return self.fc(x)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        return self.forward_once(x1), self.forward_once(x2)"
      ],
      "metadata": {
        "id": "T6gblUYkh2Zu"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# 9. Train the Siamese Network\n",
        "# ===================================================================\n",
        "siam_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
        "])\n",
        "siam_dataset = SiameseDataset(\n",
        "    img_dir=     \"/content/MOT16/train/MOT16-02/img1\",\n",
        "    gt_path=     \"/content/MOT16/train/MOT16-02/gt/gt.txt\",\n",
        "    transform=   siam_transform,\n",
        "    image_size= (16,16)\n",
        ")\n",
        "siam_loader = DataLoader(siam_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "model_siam = SiameseNetwork().to(device)\n",
        "criterion_siam = ContrastiveLoss(margin=1.0)\n",
        "optimizer_siam = Adam(model_siam.parameters(), lr=5e-4)\n",
        "\n",
        "for epoch in range(3):\n",
        "    model_siam.train()\n",
        "    running = 0.0\n",
        "    for img1, img2, lbl in siam_loader:\n",
        "        img1, img2, lbl = img1.to(device), img2.to(device), lbl.to(device)\n",
        "        out1, out2 = model_siam(img1, img2)\n",
        "        loss = criterion_siam(out1, out2, lbl)\n",
        "        optimizer_siam.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer_siam.step()\n",
        "        running += loss.item()\n",
        "    print(f\"[Siamese] Epoch {epoch+1}/3 Loss: {running/len(siam_loader):.4f}\")\n",
        "\n",
        "torch.save(model_siam.state_dict(), \"siamese_network.pth\")\n",
        "print(\"🔖 Siamese weights saved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1XmeYCMh4Kz",
        "outputId": "4cc98d69-e9c9-4770-be63-931ae2ab785f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Siamese] Epoch 1/3 Loss: 0.3204\n",
            "[Siamese] Epoch 2/3 Loss: 0.3027\n",
            "[Siamese] Epoch 3/3 Loss: 0.2952\n",
            "🔖 Siamese weights saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# 10. Inference & Tracking Pipeline\n",
        "# ===================================================================\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 10.1 Setup device & reload models\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# reload detector\n",
        "model_det = get_detector(num_classes=2)\n",
        "model_det.load_state_dict(torch.load(\"fasterrcnn_finetuned.pth\", map_location=device))\n",
        "model_det.to(device).eval()\n",
        "\n",
        "# reload Siamese\n",
        "model_siam = SiameseNetwork().to(device)\n",
        "model_siam.load_state_dict(torch.load(\"siamese_network.pth\", map_location=device))\n",
        "model_siam.eval()\n",
        "\n",
        "# 10.2 Define transforms\n",
        "det_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "siam_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
        "])\n",
        "\n",
        "# Improved tracking function with better ID assignment logic\n",
        "# Improved tracking function with proper tensor detachment\n",
        "def track_sequence(seq_dir, output_path,\n",
        "                  det_model, siam_model,\n",
        "                  det_tf, siam_tf,\n",
        "                  det_threshold=0.7,\n",
        "                  match_threshold=0.5):\n",
        "\n",
        "    # init video writer\n",
        "    frames = sorted(os.listdir(seq_dir))\n",
        "    sample = cv2.imread(os.path.join(seq_dir, frames[0]))\n",
        "    h, w = sample.shape[:2]\n",
        "    writer = cv2.VideoWriter(output_path,\n",
        "                           cv2.VideoWriter_fourcc(*\"mp4v\"),\n",
        "                           20, (w, h))\n",
        "\n",
        "    tracks = {}     # track_id -> feature vector\n",
        "    prev_boxes = {} # track_id -> previous bounding box\n",
        "    next_id = 1\n",
        "\n",
        "    for fn in frames:\n",
        "        # load frame\n",
        "        img_bgr = cv2.imread(os.path.join(seq_dir, fn))\n",
        "        img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
        "        pil_img = Image.fromarray(img_rgb)\n",
        "\n",
        "        # 1) detection\n",
        "        inp = det_tf(pil_img).to(device)\n",
        "        with torch.no_grad():  # Added no_grad here\n",
        "            preds = det_model([inp])[0]\n",
        "        boxes = preds[\"boxes\"].detach().cpu()  # Added detach\n",
        "        scores = preds[\"scores\"].detach().cpu()  # Added detach\n",
        "        mask = scores > det_threshold\n",
        "        boxes = boxes[mask]\n",
        "\n",
        "        # 2) extract Siamese features\n",
        "        feats = []\n",
        "        for b in boxes:\n",
        "            x1,y1,x2,y2 = map(int, b.tolist())\n",
        "            crop = pil_img.crop((x1,y1,x2,y2)).resize((16,16))\n",
        "            t2 = siam_tf(crop).unsqueeze(0).to(device)\n",
        "            with torch.no_grad():  # Added no_grad here\n",
        "                f = siam_model.forward_once(t2)\n",
        "                f = f / f.norm(dim=1, keepdim=True)\n",
        "            feats.append(f.detach().cpu())  # Added detach\n",
        "\n",
        "        # 3) Improved assignment using Hungarian algorithm\n",
        "        if len(feats) > 0:\n",
        "            if tracks:\n",
        "                # Create cost matrix\n",
        "                cost_matrix = np.zeros((len(feats), len(tracks)))\n",
        "                for i, feat in enumerate(feats):\n",
        "                    box = boxes[i]\n",
        "                    box_center = [(box[0] + box[2]) / 2, (box[1] + box[3]) / 2]\n",
        "\n",
        "                    for j, (tid, track_feat) in enumerate(tracks.items()):\n",
        "                        # Compute feature similarity\n",
        "                        with torch.no_grad():  # Add no_grad around the similarity computation\n",
        "                            sim = F.cosine_similarity(feat, track_feat, dim=1).item()\n",
        "\n",
        "                        # Incorporate spatial distance if we have previous box info\n",
        "                        if tid in prev_boxes:\n",
        "                            prev_box = prev_boxes[tid]\n",
        "                            prev_center = [(prev_box[0] + prev_box[2]) / 2, (prev_box[1] + prev_box[3]) / 2]\n",
        "                            dist = np.sqrt((box_center[0] - prev_center[0])**2 + (box_center[1] - prev_center[1])**2)\n",
        "                            # Normalize distance to [0, 1] range\n",
        "                            norm_dist = min(dist / 100.0, 1.0)\n",
        "                            # Combine similarity and distance (higher value = better match)\n",
        "                            sim = sim * (1 - 0.3 * norm_dist)\n",
        "\n",
        "                        # Convert similarity to cost (lower is better for Hungarian)\n",
        "                        cost_matrix[i, j] = 1.0 - sim\n",
        "\n",
        "                # Use Hungarian algorithm if scipy is available\n",
        "                try:\n",
        "                    from scipy.optimize import linear_sum_assignment\n",
        "                    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
        "\n",
        "                    # Initialize assigned IDs\n",
        "                    assigned_ids = [-1] * len(feats)\n",
        "\n",
        "                    # Process assignments based on cost threshold\n",
        "                    track_ids = list(tracks.keys())\n",
        "                    for i, j in zip(row_ind, col_ind):\n",
        "                        # Only assign if similarity is above threshold\n",
        "                        if cost_matrix[i, j] < (1.0 - match_threshold):\n",
        "                            assigned_ids[i] = track_ids[j]\n",
        "                            # Update track feature with moving average\n",
        "                            tid = track_ids[j]\n",
        "                            alpha = 0.7  # Weight for previous feature\n",
        "                            # Make sure we're doing operations on detached tensors\n",
        "                            tracks[tid] = alpha * tracks[tid] + (1 - alpha) * feats[i]\n",
        "                            # Normalize\n",
        "                            with torch.no_grad():  # Add no_grad around the normalization\n",
        "                                tracks[tid] = tracks[tid] / tracks[tid].norm(dim=1, keepdim=True)\n",
        "                            # Update previous box\n",
        "                            prev_boxes[tid] = boxes[i].tolist()\n",
        "\n",
        "                    # Assign new IDs to unassigned detections\n",
        "                    for i, aid in enumerate(assigned_ids):\n",
        "                        if aid == -1:\n",
        "                            tracks[next_id] = feats[i]\n",
        "                            prev_boxes[next_id] = boxes[i].tolist()\n",
        "                            assigned_ids[i] = next_id\n",
        "                            next_id += 1\n",
        "\n",
        "                except ImportError:\n",
        "                    # Fallback to greedy assignment if scipy is not available\n",
        "                    assigned = []\n",
        "                    for i, f in enumerate(feats):\n",
        "                        best_id, best_sim = None, match_threshold\n",
        "                        for tid, pf in tracks.items():\n",
        "                            with torch.no_grad():  # Add no_grad here\n",
        "                                sim = F.cosine_similarity(f, pf, dim=1).item()\n",
        "                            if sim > best_sim:\n",
        "                                best_sim, best_id = sim, tid\n",
        "                        if best_id is not None:\n",
        "                            tracks[best_id] = f\n",
        "                            prev_boxes[best_id] = boxes[i].tolist()\n",
        "                            assigned.append(best_id)\n",
        "                        else:\n",
        "                            tracks[next_id] = f\n",
        "                            prev_boxes[next_id] = boxes[i].tolist()\n",
        "                            assigned.append(next_id)\n",
        "                            next_id += 1\n",
        "                    assigned_ids = assigned\n",
        "            else:\n",
        "                # First frame - assign new IDs to all detections\n",
        "                assigned_ids = []\n",
        "                for i, f in enumerate(feats):\n",
        "                    tracks[next_id] = f\n",
        "                    prev_boxes[next_id] = boxes[i].tolist()\n",
        "                    assigned_ids.append(next_id)\n",
        "                    next_id += 1\n",
        "        else:\n",
        "            assigned_ids = []\n",
        "\n",
        "        # 4) draw & write\n",
        "        for b, tid in zip(boxes, assigned_ids):\n",
        "            x1,y1,x2,y2 = map(int, b.tolist())\n",
        "            # Generate consistent colors based on ID\n",
        "            color_r = (tid * 43) % 256\n",
        "            color_g = (tid * 71) % 256\n",
        "            color_b = (tid * 113) % 256\n",
        "            color = (color_b, color_g, color_r)  # BGR format for OpenCV\n",
        "\n",
        "            # box\n",
        "            cv2.rectangle(img_bgr, (x1,y1), (x2,y2), color, 2)\n",
        "\n",
        "            # label background\n",
        "            text = f\"person {tid}\"\n",
        "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "            fs = 0.5\n",
        "            th = cv2.getTextSize(text, font, fs, 1)[0][1]\n",
        "            text_width = cv2.getTextSize(text, font, fs, 1)[0][0]\n",
        "            cv2.rectangle(img_bgr,\n",
        "                        (x1, y1-th-6),\n",
        "                        (x1 + text_width + 4, y1),\n",
        "                        color, cv2.FILLED)\n",
        "\n",
        "            # text\n",
        "            cv2.putText(img_bgr, text, (x1+2, y1-4),\n",
        "                      font, fs, (255,255,255), 1)\n",
        "\n",
        "        writer.write(img_bgr)\n",
        "\n",
        "    writer.release()\n",
        "    print(f\"✅ Saved tracking video to {output_path}\")\n",
        "\n",
        "# 10.4 Run on test sequence\n",
        "test_seq = \"/content/MOT16/test/MOT16-01/img1\"\n",
        "track_sequence(\n",
        "    seq_dir=test_seq,\n",
        "    output_path=\"tracking_output.mp4\",\n",
        "    det_model=model_det,\n",
        "    siam_model=model_siam,\n",
        "    det_tf=det_transform,\n",
        "    siam_tf=siam_transform,\n",
        "    det_threshold=0.7,\n",
        "    match_threshold=0.5\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6m4E5Osh6WR",
        "outputId": "f6b5afbf-7f9d-4e34-bfa7-0641acb0d8a9"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-4e448f06158d>:97: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
            "  dist = np.sqrt((box_center[0] - prev_center[0])**2 + (box_center[1] - prev_center[1])**2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Saved tracking video to tracking_output.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BnJZIjhAIbAq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}